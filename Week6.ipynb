{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a036b648197d552e",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:55.798285Z",
     "start_time": "2025-05-07T19:32:55.794049Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import random\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b18fcf47759da5",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "the the Finnish-English translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f310f2a36d28ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:55.918044Z",
     "start_time": "2025-05-07T19:32:55.815071Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = \"./datasets/fin.txt\"\n",
    "\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, finnish, rest = line.split(\"\\t\")\n",
    "    finnish = \"[start] \" + finnish + \" [end]\"\n",
    "    text_pairs.append((finnish, english))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce8cba63cd3b1",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "Shuffle the dataset and split it into training, validation, and test sets. The portions of the dataset are 70% training, 15% validation, and 15% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f7c9cdcf9285f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:55.974233Z",
     "start_time": "2025-05-07T19:32:55.930948Z"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05afe00f0c6bb6",
   "metadata": {},
   "source": [
    "Strip the punctuation from the text and remove \"[\" and \"]\" from the punctuation list. Define a custom standardization function that converts the text to lowercase and removes the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96eac18187b8a7c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:55.995589Z",
     "start_time": "2025-05-07T19:32:55.989654Z"
    }
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a11ab00a0dc5c",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Define the vocabulary size as 15 000 and sequence length as 40. Create two TextVectorization layers, one for the source language (Finnish) and one for the target language (English). The TextVectorization layer will be used to convert the text into integer sequences. The adapt method is called on both layers to fit them to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67dad17aa7d9601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.406513Z",
     "start_time": "2025-05-07T19:32:56.018690Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 40\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_finnish_texts = [pair[0] for pair in train_pairs]\n",
    "train_english_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_finnish_texts)\n",
    "target_vectorization.adapt(train_english_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8272ad35f57f6",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "The batch size is defined to be 64. Define a function format_dataset that takes the Finnish and English text as input and returns a dictionary with the Finnish text and the English text shifted by one position. The make_dataset function creates a TensorFlow dataset from the pairs of Finnish and English text, shuffles it, and caches it for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79d086c4524febf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.769041Z",
     "start_time": "2025-05-07T19:32:56.419752Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(fin, eng):\n",
    "    fin = source_vectorization(fin)\n",
    "    eng = target_vectorization(eng)\n",
    "    return ({\n",
    "                \"finnish\": fin,\n",
    "                \"english\": eng[:, :-1],\n",
    "            }, eng[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    fin_texts, eng_texts = zip(*pairs)\n",
    "    fin_texts = list(fin_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((fin_texts, eng_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ab6f33cfb6baa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.903636Z",
     "start_time": "2025-05-07T19:32:56.781216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['finnish'].shape: (64, 40)\n",
      "inputs['english'].shape: (64, 40)\n",
      "targets.shape: (64, 40)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['finnish'].shape: {inputs['finnish'].shape}\")\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00f6798315bcd3",
   "metadata": {},
   "source": [
    "# Positional Embedding layer\n",
    "The PositionalEmbedding layer is defined to add positional information to the token embeddings. The call method computes the token and position embeddings and returns their sum. The compute_mask method creates a mask for the input sequences so that the padding tokens are ignored during training. The get_config method returns the configuration of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8fba9ccf885486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.933431Z",
     "start_time": "2025-05-07T19:32:56.928155Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"output_dim\": self.output_dim,\n",
    "        \"sequence_length\": self.sequence_length,\n",
    "        \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efded14085dcc598",
   "metadata": {},
   "source": [
    "# Encoder and Decoder Layers\n",
    "The TransformerEncoder is defined to create the encoder part of the transformer model. It uses multi-head attention and a feed-forward network. The call method computes the attention output and applies layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752c67f70e7ad7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.958284Z",
     "start_time": "2025-05-07T19:32:56.952953Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2efa509cc756de",
   "metadata": {},
   "source": [
    "TransformerDecoder is defined to create the decoder part of the transformer model. It uses multi-head attention and a feed-forward network. The call method computes the attention output and applies layer normalization. The get_causal_attention_mask method creates a causal attention mask for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0f738e45a7cf7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:56.982937Z",
     "start_time": "2025-05-07T19:32:56.974591Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        padding_mask = None\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8769c47ee68d8051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:32:57.540500Z",
     "start_time": "2025-05-07T19:32:56.996366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kone17\\anaconda3\\envs\\keras\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kone17\\anaconda3\\envs\\keras\\Lib\\site-packages\\keras\\src\\layers\\layer.py:939: UserWarning: Layer 'transformer_encoder' (of type TransformerEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 512\n",
    "num_heads = 4\n",
    "\n",
    "\n",
    "# Update encoder and decoder inputs\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"finnish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e23349e77d42b",
   "metadata": {},
   "source": [
    "# Compile And Train The Model\n",
    "The model is compiled with the RMSprop optimizer and sparse categorical crossentropy loss. The model is trained for 30 epochs with a batch size of 64. The validation data is used to evaluate the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c19697b77e1142a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:04:15.514259Z",
     "start_time": "2025-05-07T19:32:57.559598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 805ms/step - accuracy: 0.8535 - loss: 5.2361 - val_accuracy: 0.9724 - val_loss: 1.3776\n",
      "Epoch 2/3\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 782ms/step - accuracy: 0.9609 - loss: 1.1726 - val_accuracy: 0.9154 - val_loss: 0.6157\n",
      "Epoch 3/3\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 781ms/step - accuracy: 0.9671 - loss: 0.5888 - val_accuracy: 0.9695 - val_loss: 0.4083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d1e1dd940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(train_ds, epochs=3, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995649f444dbeaf",
   "metadata": {},
   "source": [
    "# Evaluate The Model\n",
    "\n",
    "Model is too large to train on a CPU. The outputs are not at all what they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbef804ace30d532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:07:02.240804Z",
     "start_time": "2025-05-07T20:06:52.291486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 1:\n",
      "[start] Ne linnut laulavat. [end]\n",
      "[start] came privacy treat i                i\n",
      "\n",
      "Translation 2:\n",
      "[start] Sinä olet niin tyhmä. [end]\n",
      "[start] me mary like homework i               homework\n",
      "\n",
      "Translation 3:\n",
      "[start] Mitä olet mieltä? Pitäisikö minun pyytää anteeksi Tomilta? [end]\n",
      "[start] this mary pretty toy my killed many bit i           \n",
      "\n",
      "Translation 4:\n",
      "[start] Onks toi sun ex-vaimo? [end]\n",
      "[start] largest making family consumption i             i  \n",
      "\n",
      "Translation 5:\n",
      "[start] Oletko koskaan nähnyt UFO:a? [end]\n",
      "[start] when go without consumption i             i  \n",
      "\n",
      "Translation 6:\n",
      "[start] Olen ollut aika kiireinen. [end]\n",
      "[start] he very long playing i             i  \n",
      "\n",
      "Translation 7:\n",
      "[start] Minulla ei ole sunnuntaisin aina vapaata. [end]\n",
      "[start] his to of abroad see crime i       i    see  abroad\n",
      "\n",
      "Translation 8:\n",
      "[start] Et koskaan ole kiireisen näköinen. [end]\n",
      "[start] were go of consumption mistake i              immediately\n",
      "\n",
      "Translation 9:\n",
      "[start] Sinun ei olisi pitänyt mennä. [end]\n",
      "[start] your to they over him i              over\n",
      "\n",
      "Translation 10:\n",
      "[start] Sinä et voi voittaa tätä. [end]\n",
      "[start] me were how fat by i              fat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = target_vectorization.get_vocabulary()\n",
    "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence]\n",
    "        )[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence]\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = eng_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_fin_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(10):\n",
    "    print(\"Translation \" + str(i + 1) + \":\")\n",
    "    input_sentence = random.choice(test_fin_texts)\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
